---
layout: default
---

<div class="home">

  <ul class="social-media-list">
    {% if site.email %}
    <li>
      {% include icon-mail.html email=site.email %}
    </li>
    {% endif %}
    {% if site.github_username %}
    <li>
      {% include icon-github.html username=site.github_username %}
    </li>
    {% endif %}
    {% if site.linkedin_username %}
    <li>
      {% include icon-linkedin.html username=site.linkedin_username %}
    </li>
    {% endif %}
  </ul>

  <div class='page-content'>

    <h1 class="page-heading">About</h1>

      <p> Hello! I'm a Computer Science Ph.D. student at Stanford University advised by <a href="https://cs.stanford.edu/people/chrismre/">Christopher Ré</a>. My research interests lie at the intersection of machine learning and systems. Recently, I have been excited about learning low-dimensional representations, or embeddings, of data, including words, source code, and knowledge graph entities.
        <!-- I am currently working on a project to improve the <i>stability</i> of embedding algorithms, a key building block of machine learning pipelines. Previously, I also worked on a project to improve the <i>efficiency</i> of training ML algorithms through low-precision arithmetic.  -->
      I graduated with a BS in Electrical & Computer Engineering and a BS in Computer Science from Cornell University. At Cornell, I had the opportunity to do research in computer architecture with <a href="http://www.csl.cornell.edu/~cbatten/">Christopher Batten</a>. I am supported by the <a href="https://www.nsfgrfp.org/">National Science Foundation Graduate Research Fellowship</a> and the <a href=https://vpge.stanford.edu/fellowships-funding/enhancing-diversity-graduate>EDGE Fellowship</a>.
    </p>

    <h1 class="page-heading">Selected Projects</h1>
    <ul>
        <li> <p><b>Embedding Stability</b>. Embeddings must be continually re-trained on constantly changing data (i.e. changing word meanings, new vocabulary for word embeddings). However, training embeddings is inherently unstable, such that small changes in data can cause dramatically different results, making debugging, repeatability, and model dependencies increasingly challenging. We study the impact of the embedding memory on this instability, and evaluate inexpensive embedding measures to predict when this instability will impact downstream natural language processing tasks.
        </li></p>

      <li> <p><b>High-Accuracy Low-Precision Training (HALP)</b>. In collaboration with <a href="http://www.cs.cornell.edu/~cdesa/">Chris De Sa</a> and others, I worked on HALP, a gradient descent variant which is able to theoretically converge to highly accurate solutions while using low-precision fixed-point arithmetic. We empirically verified HALP on linear regression and logistic regression problems, as well as LSTMs and CNNs.<a href="https://dawn.cs.stanford.edu/2018/03/09/low-precision/">[blog]</a> <a href="https://arxiv.org/abs/1803.03383">[pdf]</a> <a href={{ "/assets/halp_slides.pdf" | absolute_url }}>[slides]</a>
      </li></p>
    </ul>
    <ul>
      <li> <p><b>Proxy Kernel for RISC-V Processor</b>. In Christopher Batten's research group, I extended a RISC-V pipelined processor to support system calls via a proxy kernel. The work was done in <a href="https://github.com/cornell-brg/pymtl">PyMTL</a> (Python-based hardware modeling framework) and C.</p>
      </li>
      <li><p> <b> Neural Network Accelerator</b>. As a final project for <a href="https://web.csl.cornell.edu/courses/ece5745/">ECE 5745 Complex Digital ASIC Design</a>, I built an accelerator to classify handwritten digits. The design was pushed through the ASIC flow using Synopsys and evaluated on power, performance, and area. </li></p>
    </ul>

    <h1 class="page-heading">Publications and Pre-prints</h1>
      <ul>
          <li><p>
              Nimit Sharad Sohoni, Christopher Richard Aberger, <b>Megan Leszczynski</b>, Jian Zhang, and Christopher Ré. <b><a href=https://arxiv.org/abs/1904.10631>Low-Memory Neural Network Training: A Technical Report</a></b>. <em>arXiv Preprint</em>, Apr. 2019.
          </p>
          <li><p>
              <b>Megan Leszczynski</b>, Sen Wu, Christopher Richard Aberger, and Christopher Ré. Quantifying the Stability of Word Embeddings</b></a>. <em>In WiML at NeurIPS</em>, Dec. 2018.
          <li><p>
          Christopher De Sa, <b>Megan Leszczynski</b>, Jian Zhang, Alana Marzoev, Christopher Richard Aberger, Kunle Olukotun, and Christopher Ré. <a href="https://arxiv.org/abs/1803.03383"</a><b>High-Accuracy Low-Precision Training</b></a>. <em>arXiv Preprint</em>, Mar. 2018.
        <li><p>
          Harry Freeman, <b>Megan Leszczynski</b>, and Gargi Ratnaparkhi. iOS Controlled, Low Cost, Low Power Massage Vest Driven by PIC32. <em>Circuit Cellar</em>, Jan. 2018.</p>
        </li>
        <li><p>
          <b>Megan Leszczynski</b> and José Moreira.<b> <a href="http://phys.csail.mit.edu/papers/3.pdf">Machine Solver for Physics Word Problems</a></b>. <em>In NeurIPS Intuitive Physics Workshop</em>, Dec. 2016.</p>
        </li>
      </ul>


  <h1 class="page-heading">Teaching Experience</h1>

    <ul>
      <li> Cornell University
        <ul>
          <li>ECE 4750: Computer Architecture, Undergraduate Teaching Assistant (Fall 2016)</li>
          <li>CS 1110: Introduction to Python, Consultant (Fall 2014, Spring 2015, Fall 2015, Spring 2016)</li>
        </ul>
      </li>
    <ul>

  </div>
</div>
